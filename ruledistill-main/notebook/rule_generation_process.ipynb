{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule Generation Process\n",
    "This notebook facilitates the iterative generation of financial reasoning rules based on model failures using Ollama, followed by verification with Llama 3.3 70B.\n",
    "\n",
    "**Note:** Failed results are now indexed by their row number in `results.jsonl`, which corresponds directly to the record index in `train.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curate_dataset_tools.ipynb  rule_generation_process.ipynb\n",
      "proof_of_concept.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from /root/hsin_research/FinQA-main/dataset/train.json...\n",
      "Loading failures from /root/hsin_research/ruledistill-main/data/failed_results_with_ids.jsonl...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Adds the root directory (ruledistill-main) to the path\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "from src.rule_generator import load_dataset, load_failures, get_rule_prompt, generate_rules_for_range, verify_rule_with_llama\n",
    "\n",
    "dataset = load_dataset()\n",
    "failures = load_failures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Select Range of Failures\n",
    "Specify the indices of the rows in `failed_results_with_ids.jsonl` you want to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing failures in 'failed_results_with_ids.jsonl' slice [0:3]...\n",
      "[Failure Slot 0] Dataset Index: 0 | Q: what is the the interest expense in 2009?...\n",
      "[Failure Slot 1] Dataset Index: 1 | Q: during the 2012 year , did the equity awards in which the prescribed performance milestones were ach...\n",
      "[Failure Slot 2] Dataset Index: 2 | Q: what was the total operating expenses in 2018 in millions...\n"
     ]
    }
   ],
   "source": [
    "START_IDX = 0\n",
    "END_IDX = 2\n",
    "OLLAMA_MODEL = \"gemini-3-pro-preview:latest\"\n",
    "\n",
    "print(f\"Analyzing failures in 'failed_results_with_ids.jsonl' slice [{START_IDX}:{END_IDX+1}]...\")\n",
    "for i in range(START_IDX, END_IDX + 1):\n",
    "    fail = failures[i]\n",
    "    row_idx = fail['id']\n",
    "    item = dataset[row_idx]\n",
    "    print(f\"[Failure Slot {i}] Dataset Index: {row_idx} | Q: {item['qa']['question'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Rules via Ollama\n",
    "This step requires the `ollama` Python package and the Ollama server to be running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from /root/hsin_research/FinQA-main/dataset/train.json...\n",
      "Loading failures from /root/hsin_research/failed_results_with_ids.jsonl...\n",
      "Processing failures in 'failed_results_with_ids.jsonl' slice [0:3]...\n",
      "[0] Generating rule for row index 0...\n",
      "[1] Generating rule for row index 1...\n",
      "[2] Generating rule for row index 2...\n",
      "Error generating rule for row index 2: you've reached your premium model request limit (status code: 403)\n",
      "\n",
      "--- Rules for Dataset Index 0 ---\n",
      "**Gap Analysis**\n",
      "\n",
      "The model failed because it interpreted the context literally as only providing \"changes\" (sensitivity data) rather than absolute values. It concluded that the specific \"interest expense in 2009\" was missing (\"insufficient data\"). However, in financial reasoning, specifically regarding variable rate instruments, providing the sensitivity of interest expense to rate changes allows for the calculation of the underlying **Notional Amount** (Principal).\n",
      "\n",
      "The Ground Truth Answer (380) is derived from the formula:\n",
      "$$ \\text{Notional Amount} = \\frac{\\text{Change in Interest Expense}}{\\text{Change in Interest Rate}} $$\n",
      "Given:\n",
      "*   Change in Expense = 3.8\n",
      "*   Change in Rate = 100 basis points = 1% = 0.01\n",
      "\n",
      "Calculation: $3.8 / 0.01 = 380$.\n",
      "\n",
      "The model failed to recognize that sensitivity statements are solvable math problems for determining principal/notional balances.\n",
      "\n",
      "**Rule Synthesis**\n",
      "\n",
      "<Rule id=\"sensitivity_to_principal_inference\" phase=\"generation\" confidence=\"1\" source=\"failure_log\">\n",
      "    <Trigger>changes by .* basis points .* interest expense would change by</Trigger>\n",
      "    <Action>When context describes interest rate sensitivity (e.g., \"if LIBOR changes by X basis points, expense changes by Y\"), do not output \"insufficient data\". Calculate the implied Notional Amount (Variable Rate Debt) using the formula: Notional = Change_in_Expense / (Change_in_Basis_Points / 10000). Use this calculated Notional Amount as the answer or as a variable to derive the requested value.</Action>\n",
      "</Rule>\n",
      "\n",
      "--- Rules for Dataset Index 1 ---\n",
      "Here is the analysis and the synthesized rule.\n",
      "\n",
      "### 1. Gap Analysis\n",
      "The model failed because it treated a **Boolean comparison question** as a **numerical calculation task**.\n",
      "\n",
      "*   **The Question:** \"Did the equity awards... exceed the equity award compensation expense...?\" This requires a Yes/No answer based on a comparison of two values.\n",
      "*   **The Logic Required:**\n",
      "    1.  Calculate Total Fair Value: $607 \\text{ (shares in thousands)} \\times 18.13 = \\$11,004.91 \\text{ (in thousands)} \\rightarrow \\$11,004,910$.\n",
      "    2.  Identify Expense: $\\$3.3 \\text{ million}$.\n",
      "    3.  Compare: Is $\\$11,004,910 > \\$3,300,000$?\n",
      "    4.  Final Output: **Yes**.\n",
      "*   **The Model's Failure:** The model performed step 1 (calculating $607 \\times 18.13 \\approx 11,000$), but it stopped there. It output the numerical result of the first variable rather than completing the logic to compare it against the second variable and answering the specific question asked.\n",
      "\n",
      "### 2. Synthesize a Rule\n",
      "\n",
      "```xml\n",
      "<Rule id=\"boolean_comparison_completion\" phase=\"generation\" confidence=\"1\" source=\"failure_log\">\n",
      "    <Trigger>question starts with \"Did\", \"Does\", \"Is\" and contains comparative terms like \"exceed\", \"greater than\", \"surpass\"</Trigger>\n",
      "    <Action>Identify the two distinct values being compared. Calculate both values fully, ensuring unit consistency. Perform the arithmetic comparison and output the resulting Boolean decision (Yes/No) as the final answer, do not output the intermediate numerical value of the first entity.</Action>\n",
      "</Rule>\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "generated_rules = generate_rules_for_range(START_IDX, END_IDX, model=OLLAMA_MODEL)\n",
    "\n",
    "for entry in generated_rules:\n",
    "    print(f\"\\n--- Rules for Dataset Index {entry['row_index']} ---\")\n",
    "    print(entry['rule'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Generated Rules with Llama 3.3 70B\n",
    "Now we check if the rules actually improve the model's performance on these specific samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Gap Analysis**\n",
      "\n",
      "The model failed because it interpreted the context literally as only providing \"changes\" (sensitivity data) rather than absolute values. It concluded that the specific \"interest expense in 2009\" was missing (\"insufficient data\"). However, in financial reasoning, specifically regarding variable rate instruments, providing the sensitivity of interest expense to rate changes allows for the calculation of the underlying **Notional Amount** (Principal).\n",
      "\n",
      "The Ground Truth Answer (380) is derived from the formula:\n",
      "$$ \\text{Notional Amount} = \\frac{\\text{Change in Interest Expense}}{\\text{Change in Interest Rate}} $$\n",
      "Given:\n",
      "*   Change in Expense = 3.8\n",
      "*   Change in Rate = 100 basis points = 1% = 0.01\n",
      "\n",
      "Calculation: $3.8 / 0.01 = 380$.\n",
      "\n",
      "The model failed to recognize that sensitivity statements are solvable math problems for determining principal/notional balances.\n",
      "\n",
      "**Rule Synthesis**\n",
      "\n",
      "<Rule id=\"sensitivity_to_principal_inference\" phase=\"generation\" confidence=\"1\" source=\"failure_log\">\n",
      "    <Trigger>changes by .* basis points .* interest expense would change by</Trigger>\n",
      "    <Action>When context describes interest rate sensitivity (e.g., \"if LIBOR changes by X basis points, expense changes by Y\"), do not output \"insufficient data\". Calculate the implied Notional Amount (Variable Rate Debt) using the formula: Notional = Change_in_Expense / (Change_in_Basis_Points / 10000). Use this calculated Notional Amount as the answer or as a variable to derive the requested value.</Action>\n",
      "</Rule>\n",
      "Verifying rule for Row Index 0...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Result: STILL WRONG \u274c (computation error)\n",
      "  Original Prediction: n/a\n",
      "  New Prediction:      38.0\n",
      "  Ground Truth:        380.0\n",
      "Here is the analysis and the synthesized rule.\n",
      "\n",
      "### 1. Gap Analysis\n",
      "The model failed because it treated a **Boolean comparison question** as a **numerical calculation task**.\n",
      "\n",
      "*   **The Question:** \"Did the equity awards... exceed the equity award compensation expense...?\" This requires a Yes/No answer based on a comparison of two values.\n",
      "*   **The Logic Required:**\n",
      "    1.  Calculate Total Fair Value: $607 \\text{ (shares in thousands)} \\times 18.13 = \\$11,004.91 \\text{ (in thousands)} \\rightarrow \\$11,004,910$.\n",
      "    2.  Identify Expense: $\\$3.3 \\text{ million}$.\n",
      "    3.  Compare: Is $\\$11,004,910 > \\$3,300,000$?\n",
      "    4.  Final Output: **Yes**.\n",
      "*   **The Model's Failure:** The model performed step 1 (calculating $607 \\times 18.13 \\approx 11,000$), but it stopped there. It output the numerical result of the first variable rather than completing the logic to compare it against the second variable and answering the specific question asked.\n",
      "\n",
      "### 2. Synthesize a Rule\n",
      "\n",
      "```xml\n",
      "<Rule id=\"boolean_comparison_completion\" phase=\"generation\" confidence=\"1\" source=\"failure_log\">\n",
      "    <Trigger>question starts with \"Did\", \"Does\", \"Is\" and contains comparative terms like \"exceed\", \"greater than\", \"surpass\"</Trigger>\n",
      "    <Action>Identify the two distinct values being compared. Calculate both values fully, ensuring unit consistency. Perform the arithmetic comparison and output the resulting Boolean decision (Yes/No) as the final answer, do not output the intermediate numerical value of the first entity.</Action>\n",
      "</Rule>\n",
      "```\n",
      "Verifying rule for Row Index 1...\n",
      "  Result: STILL WRONG \u274c (unknown (missing gt))\n",
      "  Original Prediction: 11000.0\n",
      "  New Prediction:      n/a\n",
      "  Ground Truth:        n/a\n"
     ]
    }
   ],
   "source": [
    "verification_results = []\n",
    "for entry in generated_rules:\n",
    "    row_idx = entry['row_index']\n",
    "    rule_text = entry['rule']\n",
    "    print(rule_text)\n",
    "    orig_fail = entry['original_failure']\n",
    "    \n",
    "    print(f\"Verifying rule for Row Index {row_idx}...\")\n",
    "    res = verify_rule_with_llama(dataset[row_idx], rule_text)\n",
    "    \n",
    "    if res:\n",
    "        print(f\"  Result: {'CORRECT \u2705' if res['is_correct'] else 'STILL WRONG \u274c'} ({res['error_category']})\")\n",
    "        print(f\"  Original Prediction: {orig_fail['parsed_prediction']}\")\n",
    "        print(f\"  New Prediction:      {res['parsed_prediction']}\")\n",
    "        print(f\"  Ground Truth:        {res['ground_truth']}\")\n",
    "        verification_results.append({\n",
    "            \"row_index\": row_idx,\n",
    "            \"original\": orig_fail,\n",
    "            \"new_verification\": res\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Final Session Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f\"rule_verification_session_{START_IDX}_{END_IDX}.json\"\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump({\n",
    "        \"generated_rules\": generated_rules,\n",
    "        \"verification_results\": verification_results\n",
    "    }, f, indent=2)\n",
    "print(f\"Session results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract <<Rulebook ... </Rulebook> from @synthesized_rules.json\n",
    "@synthesized_rules.json is a json file that contains the rules that share\n",
    "the following format: \n",
    "{\n",
    "    \"dataset_id\": int,\n",
    "    \"input_question\": str,\n",
    "    \"reasoning\": str,\n",
    "    \"generated_rule_response\": str\n",
    "},\n",
    "and we need to extract all the rules from it.\n",
    "\n",
    "First of all, the generated_rule_response is a string that contains the xml,\n",
    "but some of it has continated string that we need to extract <Rulebook ... </Rulebook> from it to get the xml.\n",
    "\n",
    "And the <Rulebook> tag has field in it, like <Rulebook domain=\\\"finqa_reasoning\\\">\\n\\n    <Rule id=\\\"01\\\" phase=\\\"generation\\\" confidence=\\\"1\\\" source=\\\"log_01\\\" type=\\\"Arithmetic Hallucination\\\">\\n\\n\n",
    "Separate different rule with same tags into different files.\n",
    "\n",
    "The directory should be ruledistill-main/data/rulebook\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Define paths relative to the notebook location\n",
    "json_path = '../src/synthesized_rules.json'\n",
    "output_dir = '../data/rulebook'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Loading rules from {json_path}...\")\n",
    "try:\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {json_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading JSON: {e}\")\n",
    "else:\n",
    "    print(f\"Found {len(data)} entries.\")\n",
    "    \n",
    "    # Regex to extract <Rulebook ...> </Rulebook> including attributes and multiline content\n",
    "    # Using dotall flag (re.S) so . matches newlines\n",
    "    rulebook_pattern = re.compile(r'(<Rulebook.*?</Rulebook>)', re.S)\n",
    "    \n",
    "    count = 0\n",
    "    for entry in data:\n",
    "        dataset_id = entry.get('dataset_id')\n",
    "        generated_response = entry.get('generated_rule_response', '')\n",
    "        \n",
    "        if dataset_id is None:\n",
    "            print(\"Skipping entry with missing dataset_id\")\n",
    "            continue\n",
    "            \n",
    "        # Find the xml block\n",
    "        match = rulebook_pattern.search(generated_response)\n",
    "        if match:\n",
    "            xml_content = match.group(1)\n",
    "            \n",
    "            # Construct filename\n",
    "            filename = f\"rulebook_{dataset_id}.xml\"\n",
    "            file_path = os.path.join(output_dir, filename)\n",
    "            \n",
    "            # Write to file\n",
    "            with open(file_path, 'w') as f:\n",
    "                f.write(xml_content)\n",
    "            \n",
    "            count += 1\n",
    "        else:\n",
    "            print(f\"Warning: No <Rulebook> tag found for dataset_id {dataset_id}\")\n",
    "            \n",
    "    print(f\"Successfully extracted {count} rulebooks to {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}